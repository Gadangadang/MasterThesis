\section{Other tools and algorithms}
\subsection*{Adaptable moment estimation (ADAM) Optimizer}\label{sec:adam}
Stochastic gradient descent, though very useful, lack the ability to adapt to the feature space. One algorithm that address this
issue is the ADAM optimizer\cite{ADAM:opti}. The Adaptable Moment Estimation (ADAM) algorithm uses stochastic gradient descent, but with 
an adaptive learning rate. This learning rate is adjusted by calculating estimates for the first and second moment\footnote{In statistics
the first moment is the expectation value for a distribution, $E[X-\mu]$. The second moment is the 
expectation value of the distribution squared, i.e the variance, $E[(X-\mu)^2]$}. Thus, a large gradient would indicate proximity 
to a minimum in feature space, thus a lower learning rate would yield a more accurate result. A small gradient would suggest far 
proximity to a local minimum, and thus a larger learning rate would increase the chance 
of approaching a minimum.\par 


\subsection*{Activation functions}
Several activation functions are used in neural networks, and how one chooses the best combination for a given problem is not trivial. 
This often leads to the use of tuning. In bulletlist \ref{item:activation_functions} we have the activation functions used in this thesis, 
and their mathematical definitions. 

\begin{enumerate}\label{item:activation_functions}
    \item  $sigmoid(x) = \frac{1}{1+e^{-x}}$
    \item $tanh(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}}$
    \item $ReLU(x) = \max(0,x)$
    \item $LeakyReLU(\alpha, x) = \max(\alpha x,x)$
    \item $Softmax(x_j) = \frac{e^{x_j}}{\sum_{i=1}^n e^{x_i}}$
    \item $Linear(x) = x$
\end{enumerate}


\subsection*{Statistical significance}
In the frequentist statistics, the Poisson distribution can be approximated with a Gaussian distribution in the limit of large number of events\cite{cowan}. 
The expression for the expected significance is given then as 
\begin{equation}\label{eq:significance_large}
    Z = \frac{s}{\sqrt{b}},
\end{equation}
where s is the amount of signal, and b is the amount of background. For low statistics we have that the significance is given as 
\begin{equation}\label{eq:significance_small}
    Z = \sqrt{2\left[(s+b)\ln(1+\frac{s}{b})-s\right]}.
\end{equation}
It can be shown that in the limit where $s << b$, the two expressions are approimately the same\cite{cowan}.

