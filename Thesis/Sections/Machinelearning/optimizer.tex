\subsection*{PRELIM TITLE Other tools and algorithms}
\subsubsection*{ADAM Optimizer}
Stochastic gradient descent, though very useful, lack the ability to adapt to the feature space. One algorithm that address this
issue is the ADAM optimizer\cite{ADAM:opti}. The ADAM (Adaptable moment estimation) uses stochastic gradient descent, but with 
an adaptiv learning rate. This learning rate is adjusted by calculating estimates for the first and second moment\footnote{In statistics
the first moment is the expectation value for a distribution, $E[X-\mu]$. The second moment is the 
expectation value of the distribution squared, i.e the variance, $E[(X-\mu)^2]$}. Thus, a large gradient would indicate close proximity 
to a minima in feature space, thus a lower learning rate would yield a more accurate result, 
where as a small gradient would suggest far proximity to a local minima, and thus a larger learning rate would increase the chance 
of approaching a minima.\par 

\subsubsection*{Hyperband}
Hyperband is a tool for hyperparameter optimization\cite{hyperband:opt}. Hyperparameter optimization is of high importance in the 
search for ideal structures and architectures when using neural networks, as there is not a way to find an a priori setup for a 
given problem. Several algorithms are used, from random search, grid search, and bayesian optimization. Hyperband is an algorithm 
proposed by L. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh and A. Talwalkar. It focuses on using successful halving\cite{successivehalving}
but at the same time doing a gridsearch for how to allocate resources. Successive halving focuses on testing n configurations, and removing 
the bottom half, thus (hopefully) quickly converging to the ideal combination. However, it is not easy to a priori know which 
number of configurations n, and how much resources one needs, r, to quickly find the ideal set. This is where Hyperband comes in. 
In essence, it fetches tries different combinations of r resources (time, data set subsampling or feature subsampling) and n 
configurations, to determine the ideal set of hyperparameters via successive halving, yeilding 5x to 30x speedup compared to 
Bayesian optimization. One drawback for this algorithm is that you cannot guarantiy that the configuration is optimal, 
but rather that it is good enough. 

%\subsubsection*{Regularization}
