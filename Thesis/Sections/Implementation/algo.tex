\section{Code implementation}

The machine learning analysis was written with Keras\cite{chollet2015keras} using the Tensorflow api\cite{tensorflow2015-whitepaper} . 
The machine learning structure was written using a functional structure\footnote{Functional structure uses a function call for layers, i.e for layers a,b, then b(a) will connect the two layers, and equals a sequential link a $\to$ b. This allows for more flexible structures. More on the functional api can be found \href{https://www.tensorflow.org/guide/keras/functional}{here}.}.
In practise, this model could just as well have been written as a Sequential model\footnote{Sequential structure adds layers in sequence, i.e for layers a, b, c we have that a $\to$ b $\to$ c, with a strict structure. This allows for more organized code. More on sequential models can be found \href{https://www.tensorflow.org/guide/keras/sequential_model}{here}.}, 
but at a cost of flexibility and lack of potential non-linear structure in the architecture. The code consists of one general class for
 the autoencoder, where the different testing cases are different classes inheriting from the parent class.\par


%Tuning \cite{omalley2019kerastuner}, \par 
%Tuning optimization \cite{hyperband:opt}, \par
%Optimizing \cite{ADAM:opti}, \par 
%Splitting and scaling \cite{scikit-learn}, \par
%Plotting \cite{Hunter:2007}, \par
%More plotting  \cite{Waskom2021}, \par
%Even more plotting \cite{plotly}, \par
%Arrays \cite{harris2020array}, \par 
%Dataframe structure \cite{reback2020pandas}, \par


\subsection*{Construction of a neural network in Tensorflow}

Using the functional structure, a general neural network in the Tensorflow API can be constructed as shown below. 
\begin{lstlisting}[language=Python, style=pythonstyle, label={code:python_func_example_general}]
import tensorflow as tf


inputs = tf.keras.layers.Input(shape=data_shape, name="input")

# First hidden layer
First_layer = tf.keras.layers.Dense(
    units=30,
    activation="relu"
)(inputs)

# Second hidden layer
Second_layer = tf.keras.layers.Dense(
    units=45, 
    activation="relu"
)(First_layer)

# Second hidden layer
output_layer = tf.keras.layers.Dense(
    units=1, 
    activation="sigmoid"
)(Second_layer)


# Model definition
nn_model = tf.keras.Model(inputs, output_layer, name="nn_model")

hp_learning_rate = 0.0015
optimizer = tf.keras.optimizers.Adam(hp_learning_rate)
nn_model.compile(loss="mse", optimizer=optimizer, metrics=["mse"]) 
\end{lstlisting}
The neural network here contains one input layer, two hidden layers, and an output layer. The choice of nodes and activation functions are 
arbitrary here as the use case has not been defined. Note that this is exactly the same as the previous example, but using the sequential structure.


\begin{lstlisting}[language=Python, style=pythonstyle, label={code:python_seq_example}]
import tensorflow as tf

nn_model = tf.keras.Sequential(
    [
        tf.keras.layers.Dense(30, activation="relu", input_shape=data_shape),
        tf.keras.layers.Dense(45, activation="relu"),
        tf.keras.layers.Dense(1, activation="sigmoid"),
    ]
)

hp_learning_rate = 0.0015
optimizer = tf.keras.optimizers.Adam(hp_learning_rate)
nn_model.compile(loss="mse", optimizer=optimizer, metrics=["mse"]) 
\end{lstlisting}

