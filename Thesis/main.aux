\relax 
\providecommand\zref@newlabel[2]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{JHEP}
\babel@aux{UKenglish}{}
\@writefile{toc}{\contentsline {chapter}{Introduction}{1}{chapter*.4}\protected@file@percent }
\citation{anom_detec}
\citation{anom_detec}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Machine learning}{3}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Chap:MLphenom}{{1}{3}{Machine learning}{chapter.1}{}}
\newlabel{Chap:MLphenom@cref}{{[chapter][1][]1}{[1][3][]3}}
\citation{FYSSTK}
\citation{FYSSTK}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Simple neural network diagram drawm using Draw.io. Here the blue dots are the input layer, the green dots are a hidden layer, and the red dots are the output layer. The arrows shows the connections between each node. \relax }}{4}{figure.caption.8}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:nndiagram}{{1.1}{4}{Simple neural network diagram drawm using Draw.io. Here the blue dots are the input layer, the green dots are a hidden layer, and the red dots are the output layer. The arrows shows the connections between each node. \relax }{figure.caption.8}{}}
\newlabel{fig:nndiagram@cref}{{[figure][1][1]1.1}{[1][4][]4}}
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces Notation\relax }}{5}{table.caption.9}\protected@file@percent }
\newlabel{tab:notation}{{1.1}{5}{Notation\relax }{table.caption.9}{}}
\newlabel{tab:notation@cref}{{[table][1][1]1.1}{[1][4][]5}}
\@writefile{lot}{\contentsline {table}{\numberline {1.2}{\ignorespaces Table containing notation used for deriving the mathematical formulas for the neural network \cite  {FYSSTK}\relax }}{5}{table.caption.9}\protected@file@percent }
\citation{FYSSTK}
\citation{FYSSTK}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Figures showing different choice of learning rate for a given costfunction, with respect to the tunable parameters. Source: \href  {https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png}{Jeremy Jordan}, accessed 03.10.22.\relax }}{6}{figure.caption.11}\protected@file@percent }
\newlabel{fig:lr_choice}{{1.2}{6}{Figures showing different choice of learning rate for a given costfunction, with respect to the tunable parameters. Source: \href {https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png}{Jeremy Jordan}, accessed 03.10.22.\relax }{figure.caption.11}{}}
\newlabel{fig:lr_choice@cref}{{[figure][2][1]1.2}{[1][6][]6}}
\citation{backprop}
\citation{Goodfellow-et-al-2016}
\newlabel{eq:localgradient}{{1.3}{8}{Backpropagation}{equation.1.0.3}{}}
\newlabel{eq:localgradient@cref}{{[equation][3][1]1.3}{[1][8][]8}}
\newlabel{eq:dzl1dz}{{1.4}{8}{Backpropagation}{equation.1.0.4}{}}
\newlabel{eq:dzl1dz@cref}{{[equation][4][1]1.4}{[1][8][]8}}
\newlabel{eq:localgradient2}{{1.5}{8}{Backpropagation}{equation.1.0.5}{}}
\newlabel{eq:localgradient2@cref}{{[equation][5][1]1.5}{[1][8][]8}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Figure depicting a model for an image denoising autoencoder. Here the input $\bf  {x}$ is the original image, $\bf  {\tilde  {x}}$ is a noised version of $\bf  {x}$, $E$ is the encoder, $D$ is the decoder, and $c$ is the latent space. Found 27.09.22 \href  {https://miro.medium.com/max/720/0*ECdHu2yeal38Jl3P.png}{here}. \relax }}{9}{figure.caption.15}\protected@file@percent }
\newlabel{fig:ae_denoise}{{1.3}{9}{Figure depicting a model for an image denoising autoencoder. Here the input $\bf {x}$ is the original image, $\bf {\tilde {x}}$ is a noised version of $\bf {x}$, $E$ is the encoder, $D$ is the decoder, and $c$ is the latent space. Found 27.09.22 \href {https://miro.medium.com/max/720/0*ECdHu2yeal38Jl3P.png}{here}. \relax }{figure.caption.15}{}}
\newlabel{fig:ae_denoise@cref}{{[figure][3][1]1.3}{[1][8][]9}}
\newlabel{eq:loss_ae}{{1.6}{9}{Autoencoders}{equation.1.0.6}{}}
\newlabel{eq:loss_ae@cref}{{[equation][6][1]1.6}{[1][9][]9}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Standard model phenomenology}{11}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Chap:SM}{{2}{11}{Standard model phenomenology}{chapter.2}{}}
\newlabel{Chap:SM@cref}{{[chapter][2][]2}{[1][11][]11}}
\citation{ROOT}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Implementation}{13}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Chap:implementation}{{3}{13}{Implementation}{chapter.3}{}}
\newlabel{Chap:implementation@cref}{{[chapter][3][]3}{[1][13][]13}}
\newlabel{code:cpp_func_example}{{3}{13}{}{lstlisting.3.-1}{}}
\newlabel{code:cpp_func_example@cref}{{[chapter][3][]3}{[1][13][]13}}
\citation{Chekanov_2019}
\newlabel{code:python_func_example}{{3}{14}{}{lstlisting.3.-2}{}}
\newlabel{code:python_func_example@cref}{{[chapter][3][]3}{[1][13][]14}}
\newlabel{eq:rmmmatrix}{{3.1}{14}{RMM matrix}{equation.3.0.1}{}}
\newlabel{eq:rmmmatrix@cref}{{[equation][1][3]3.1}{[1][14][]14}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Results}{17}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Chap:results}{{4}{17}{Results}{chapter.4}{}}
\newlabel{Chap:results@cref}{{[chapter][4][]4}{[1][17][]17}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Discussion}{19}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Chap:discussion}{{5}{19}{Discussion}{chapter.5}{}}
\newlabel{Chap:discussion@cref}{{[chapter][5][]5}{[1][19][]19}}
\@writefile{toc}{\contentsline {chapter}{Conclusion}{21}{chapter*.27}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Appendices}{23}{section*.28}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Appendix A}{25}{appendix*.29}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{Appendix B}{27}{appendix*.30}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{Appendix C}{29}{appendix*.31}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibdata{bibliography.bib}
\@writefile{toc}{\contentsline {chapter}{Appendix D}{31}{appendix*.32}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{anom_detec}{{1}{}{{}}{{}}}
\bibcite{FYSSTK}{{2}{}{{}}{{}}}
\bibcite{backprop}{{3}{}{{}}{{}}}
\bibcite{Goodfellow-et-al-2016}{{4}{}{{}}{{}}}
\bibcite{ROOT}{{5}{}{{}}{{}}}
\bibcite{Chekanov_2019}{{6}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\gdef \@abspage@last{45}
